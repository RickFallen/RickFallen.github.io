<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex3]]></title>
    <url>%2F2019%2F01%2F29%2Fml-ex3%2F</url>
    <content type="text"><![CDATA[lrCostFunction.m 这道题在上一个练习里已经做过,不再赘述: 123456h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta(2:end) .^ 2);theta_tmp = theta;theta_tmp(1) = 0;grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_tmp; 完成 oneVsAll.m 这道题的目的是通过fmincg算法训练出针对每一个分类(这里是1-10)所对应的\(\theta_0\)到\(\theta_n\)值,然后用这些\(\theta\)值去识别新的图片 我们的特征矩阵是5000x400,每一行代表一个图片,加上一列\(x_0\)就是5000x401,针对10个分类,我们最后要返回的all_theta是10x401的矩阵,所以我们要对所有分类(1-10)进行循环并组合成all_theta. all_theta第一行代表数字1经过训练后所对应的所有\(\theta\)值,第二行代表数字2经过训练后所对应的所有\(\theta\)值 ... 第10行代表数字0经过训练后所对应的所有\(\theta\)值 12345678for c = 1:num_labels initial_theta = zeros(n + 1, 1); options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50); [theta] = ... fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ... initial_theta, options); all_theta(c,:) = theta&apos;; endfor 完成 predictOneVsAll.m 得到了所有分类(对应1-10的数字)的\(\theta\)值后,接下来就要利用这些值去识别新的图片了 我们的\(X\)矩阵是5000x401,all_theta矩阵是10x401.很显然我们要用矩阵\(X\)与all_theta矩阵的转置相乘,再代入sigmoid函数,得到的是5000 x 10的矩阵,行数就是图片张数,第一列代表的是图片为1时的概率,第二列代表的是图片为2的概率...第十列代表的是图片为0概率 最后要做的是取每一行中概率最大的那一列就是图片最终识别结果 12h = sigmoid(X * all_theta&apos;);[max_num,p] = max(h , [] , 2); 注:max函数第3个参数传1代表求每一列的最大值,最后返回的是行向量.第3个参数传2代表求每一行的最大值,最后返回的是列向量.max函数第一个返回值代表最大的数是几,第二个参数代表这个最大数的下标 predict.m 之前的题目是用逻辑回归识别手写图片,这道题是用神经网络(Neural Network)识别手写图片 相比逻辑回归的线性classifier,NN可以执行更加复杂的任务,在这个练习中,我们要用Feedforward Propagation算法实现一个3层神经网络,算法如下图所示: 因此,代码如下: 123456789a_1 = [ones(m,1) , X];z_2 = a_1 * Theta1&apos;;a_2 = [ones(m,1),sigmoid(z_2)];z_3 = a_2 * Theta2&apos;;a_3 = sigmoid(z_3);[max_num,p] = max(a_3,[],2); 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex2]]></title>
    <url>%2F2019%2F01%2F28%2Fml-ex2%2F</url>
    <content type="text"><![CDATA[sigmoid.m 第一题比较简单,让我们实现Sigmoid函数,函数如下: \[ g(z)=\frac{1}{1+e^{-z}} \] 1g = 1 ./ (1 + exp(-z)); 完成 costFunction.m (逻辑回归的代价函数) 逻辑回归的hypothesis函数如下 \[ h_\theta(x)=g(\theta^Tx) \] 其中\(g\)就是我们刚才实现的sigmoid函数,逻辑回归的代价函数如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) \] 为什么这么复杂呢...因为这个函数可以把我们的代价函数变成非凸函数(non-convex),这样在求导的时候就不会出现local minimum 有了ex-1的基础,用octave/matlib实现也不是那么困难了,代码如下 12h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)); 逻辑回归的梯度下降与线性回归的梯度下降公式长的一样: \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 注:下标\(j\)代表第几个特征 需要注意的是,虽然它们公式长的一样,但是注意\(h_\theta(x)\)的实现方式是不同的,代码实现如下: 1grad = 1 / m * (X&apos; * (h - y)); 完成 predict.m 也是一道简单的题,直接上结果 12h = sigmoid(X * theta);p = h &gt;= 0.5; costFunctionReg.m 这道题主要是代价函数的正则化,用来解决过拟合(over-fitting)的问题,算法如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2 \] 其中\(m\)代表数据的条数,\(n\)代表特征个数,需要注意的是,正则化是不包括\(\theta_0\)的 关于向量的平方和已经在上一篇ex-1中介绍过了,因此代码实现如下: 123h = sigmoid(X * theta);theta_temp = theta(2:end);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta_temp .^ 2); 接下来就要计算梯度值了,跟计算代价函数一样,计算梯度值的时候对\(\theta_0\)不做正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 对于\(\theta_0\)之外的参数,需要加上正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j \] 因此,在计算的时候,我们可以手动的给\(\theta_0\)赋值为0,这样就可以用一个公式来计算,不用判断是否为\(\theta_0\),计算梯度的代码如下: 12theta_temp = [0;theta_temp];grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_temp; 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论读书]]></title>
    <url>%2F2019%2F01%2F28%2Fof_study%2F</url>
    <content type="text"><![CDATA[读书足以怡情，足以傅彩，足以长才。其怡情也，最见于独处幽居之时；其傅彩也，最见于高谈阔论之中；其长才也，最见于处世判事之际。练达之士虽能分别处理细事或一一判别枝节，然纵观统筹、全局策划，则舍好学深思者莫属。读书费时过多易惰，文采藻饰太盛则矫，全凭条文断事乃学究故态。读书补天然之不足，经验又补读书之不足，盖天生才干犹如自然花草，读书然后知如何修剪移接；而书中所示，如不以经验范之，则又大而无当。有一技之长者鄙读书，无知者羡读书，唯明智之士用读书，然书并不以用处告人，用书之智不在书中，而在书外，全凭观察得之。读书时不可存心诘难作者，不可尽信书上所言，亦不可只为寻章摘句，而应推敲细思。书有可浅尝者，有可吞食者，少数则须咀嚼消化。换言之，有只须读其部分者，有只须大体涉猎者，少数则须全读，读时须全神贯注，孜孜不倦。书亦可请人代读，取其所作摘要，但只限题材较次或价值不高者，否则书经提炼犹如水经蒸馏、淡而无味矣。 读书使人充实，讨论使人机智，笔记使人准确。因此不常作笔记者须记忆特强，不常讨论者须天生聪颖，不常读书者须欺世有术，始能无知而显有知。读史使人明智，读诗使人灵秀，数学使人周密，科学使人深刻，伦理学使人庄重，逻辑修辞之学使人善辩：凡有所学，皆成性格。人之才智但有滞碍，无不可读适当之书使之顺畅，一如身体百病，皆可借相宜之运动除之。滚球利睾肾，射箭利胸肺，慢步利肠胃，骑术利头脑，诸如此类。如智力不集中，可令读数学，盖演题须全神贯注，稍有分散即须重演；如不能辨异，可令读经院哲学，盖是辈皆吹毛求疵之人；如不善求同，不善以一物阐证另一物，可令读律师之案卷。如此头脑中凡有缺陷，皆有特药可医。 1注:中学时背的滚瓜烂熟的课文,过了N多年之后再来看这些又有了新解.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>论读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex1]]></title>
    <url>%2F2019%2F01%2F22%2Fml-ex1%2F</url>
    <content type="text"><![CDATA[warmUpExercise.m 第一题很简单,跟标题一样,是个热身题.要求生成一个5x5的单位矩阵 1A = eye(5); 完成 computeCost.m (计算单变量代价函数) 回想一下我们的单变量hypothesis公式 \[ h(\theta) = \theta_0 + \theta_1x \] 在这个公式中\(x\)指的是变量(variable),\(\theta_0\)和\(\theta_1\)是参数(parameter),之后的工作就是对代价函数(cost function)求导,找出使得代价函数最小(收敛时)\(\theta_0\)和\(\theta_1\)的值 刚开始学的时候有一段时间变量(variable)和参数(parameter)傻傻分不清 \(X\)是我们的数据矩阵,如下 \[ \left[ \begin{matrix} 3\\ 5\\ 8 \end{matrix} \right] \] 每一列代表一个特征,由于我们是单变量代价函数,所有X只有一列,为了不失一般性,可以改写我们的hypothesis公式 \[ h(\theta) = \theta_0x_0+ \theta_1x_1 \] 其中\(x_0\)的值始终为1,在我们的编程作业中\(\theta\)是一个向量代表\(\theta_1\)和\(\theta_2\),如下 \[ \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 为了用矩阵的乘法表示hypothesis公式,我们需要手动给\(X\)加上一列,如下(在ml编程作业中,已经帮我们添加了第一列1,所以无需我们再手动添加) \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] \] 然后执行矩阵的乘法 \[ h = X * \theta^T \] \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 代价函数 \[ J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}))^2 \] hypothesis计算完成后得到的是个m x 1的向量,代表的是我们通过h公式计算出的预测结果,m代表数据的行数,也就是\(X\)矩阵的行数.\(y\)向量代表的是真实的结果,所以\((h_\theta(x^{(i)}) - y^{(i)}))\)可以直接用两矩阵相减得出,即\(h - y\) 下面要做的就是计算平方和了,计算平方和有两种方法: \(\alpha^T * \alpha\) 永乐大帝在上,受小弟一拜 利用octave/matlib内置矩阵运算方法,先同时给所有数平方,再sum 于是,我们就可以得出我们的代价函数 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); 完成. gradientDescent.m (梯度下降) 为了使我们的线性方程方程\(h\)更贴近我们的数据集,因此我们要不断的去改变\(\theta_0\)和\(\theta_1\)这两个参数的值,使得代价函数\(J(\theta)\)取得最小值,如何取得最小值,我们用的是批量梯度下降算法(batch gradient descent algorithm) 回顾我们的批量梯度下降算法,即同时对每一个\(\theta\)求偏导,如下 \[ \theta = \theta - \alpha\cdot\frac{\partial}{\partial\theta}J(\theta) \] 即 \[ \theta_j = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j \] 由于我们是单变量,所以j只能取0,1,而且每次迭代我们必须要同时更新\(\theta_0\)和\(\theta_1\) (simultaneously update \(\theta_j\) for all j) 要计算 \((h_\theta(x^{(i)})-y^{(i)})\)的和,我们可以使用矩阵的乘法,由上文知\(X\)矩阵为m x 2 , \((h_\theta(x^{(i)})-y^{(i)})\) 为m维向量,则 \[ \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j = X^T*(h_\theta(x^{(i)})-y^{(i)}) = \left[ \begin{matrix} 1 &amp; 1 &amp; 1\\ 3 &amp; 5 &amp; 8\\ \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \\ 0 \end{matrix} \right] \] 可以简单的理解为\(X^T\)矩阵的第一行就是计算\(\theta_0\),第二行就是计算\(\theta_1\),第n行就是计算\(\theta_n\)(n &gt; 1就是多变量线性回归了,在这里我们的n = 1,也就是单变量线性回归) 因此,我们的代码可以这么写 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)); 完成 至此,我们的必修部分的编程作业都已做完,下面来看选修部分的作业 featureNormalize.m (特征标准化) 特征标准化的目的就是为了提升梯度下降的计算速度. 特征标准化包含两个方面: Feature Scaling Mean Normalize 它们将尝试使所有的特征都尽量放缩到-1到1之间(不严格,可有误差) 最简单的实现方式是\(x_n=\frac{x_n-\mu_n}{s_n}\),其中\(u_n\)是平均值,\(s_n\)是标准差 在octave中,一个mxn矩阵是可以和另一个m行的列向量或者n列的行向量相加减的,所以答案如下 123mu = mean(X);sigma = std(X);X_norm = (X_norm - mu) ./ sigma; 完成 computeCostMulti.m 由于矩阵乘法的原因,多变量线性回归的代价函数代码和单变量线性回归的代价函数代码一模一样,不再赘述 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); gradientDescentMulti.m 同理,多变量梯度下降的代码和单变量梯度下降的代码一样,不再赘述 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)) ; 完成 normalEqn.m(正规方程) 正规方程适用于特征个数不是特别多的场景(\(x&lt;10,000\)) , 优点: 不用迭代,一次即可计算出结果 不用选取学习率\(\alpha\)(learning rate) 不需要特征放缩 缺点: - 对于特征值个数\(x&gt;10,000\)时计算速度缓慢 - 只适用于线性回归模型,不适用于逻辑回归模型 - 可能出现矩阵不可逆的情况 公式: \[ (X*X^T)^{-1}X^Ty \] 由于公式本来就是用矩阵表示,所以用octave/matlib实现起来一点都不难,代码如下: 1theta = pinv(X&apos; * X) * X&apos; * y; 完成 最后,提交作业]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex1</tag>
      </tags>
  </entry>
</search>
