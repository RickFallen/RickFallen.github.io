<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring 总结]]></title>
    <url>%2F2019%2F08%2F14%2Fspring%2F</url>
    <content type="text"><![CDATA[Spring Family个人总结 Q&amp;A Spring Framework BeanFactory 和 ApplicationContext 的区别 Feature BeanFactory ApplicationContext Bean instantiation/wiring Yes Yes Integrated lifecycle management No Yes Automatic BeanPostProcessorregistration No Yes Automatic BeanFactoryPostProcessorregistration No Yes Convenient MessageSource access (for internalization) No Yes Built-in ApplicationEvent publication mechanism No Yes 简而言之, BeanFactory 只是加载了需要注入的 bean 而已,并没有注册后置处理器以及其他消息和事件管理 BeanFactory 和 FactoryBean 的区别 FactoryBean定义 : Interface to be implemented by objects used within a BeanFactory which are themselves factories for individual objects. If a bean implements thisinterface, it is used as a factory for an object to expose, not directly as abean instance that will be exposed itself. BeanFactory定义 : The root interface for accessing a Spring bean container. This is the basic client view of a bean container; 二者其实没有什么联系,BeanFactory代表的是Spring的IOC容器. 而FactoryBean则是工厂模式的实现,并且以Spring命名风格命名 ,比如UserFactory,在Spring里就叫做UserFactoryBean. Spring IOC容器如何解决循环依赖 使用三级缓存解决循环依赖 (按照存放的先后顺序) singletonFactories earlySingletonObjects singletonObjects 1234567public class A&#123; private B b;&#125;public class B&#123; private A a;&#125; createBeanInstance(beanName, mbd, args) 首先,调用A的构造器初始化A addSingletonFactory 把还未初始化变量的A放入singletonFactories中 populateBean(beanName, mbd, instanceWrapper) 给A对象的字段赋值 发现A中需要注入B getBean() 初始化B createBeanInstance(beanName, mbd, args) 调用B的构造器初始化B addSingletonFactory 把B放入singletonFactories中 populateBean(beanName, mbd, instanceWrapper) 发现B中需要注入A getBean() 去找A 发现A在singletonFactories 中 ,直接返回A 哪些循环依赖是不能被解决的 构造器中的循环依赖是不能被解决的 因为 Spring 会调用构造器初始化 Bean, 然后才会把 bean 实例放入解决循环依赖的缓存中. 没有了缓存, 执行创建逻辑就会陷入死循环. 为什么 getBean 的时候要用三级缓存, 二级缓存就解决不了吗 实际上 singletonFactories 和 earlySingletonObjects 都是暴露bean的早期引用的缓存, 这两者从宏观角度上是可以合二为一的, 但是 Spring 预留了可扩展性, 在 AbstractAutowireCapableBeanFactory 的doCreateBean 方法中,留意如下代码 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); 进入 getEarlyBeanReference 方法中,我们发现对于实现了 SmartInstantiationAwareBeanPostProcessor 接口的 Bean, 会进一步的对bean进行封装处理(比如返回 bean 的代理对象) 又根据单一职责的设计原则, 早期引用缓存这才被划分为了singletonFactories 和 earlySingletonObjects @Lazy 是如何实现的 ​ 在解析被 @Lazy 标记的 Bean 时, 会给相应的 BeanDefinition 对象里 lazy 属性设置为 true , 因而在执行初始化单例对象时, 会忽略掉被 @Lazy 标注的 Bean. ​ 在真正的用到被 @Lazy 标注的 Bean 时, 会直接调用 getBean 执行创建逻辑. ​ 当 @Lazy 标记的 Bean 被其他组件引用时, @Lazy 失效 SpringMVC 是如何解析方法中的注解,并赋值的 SpringMVC @RequestParam 是如何实现的 @PostConstruct 是如何实现的 CommonAnnotationBeanPostProcessor 是 InitDestroyAnnotationBeanPostProcessor 的子类 doCreateBean initializeBean InitDestroyAnnotationBeanPostProcessor.applyBeanPostProcessorsBeforeInitialization invokeInitMethods Bean 的初始化赋值顺序 doCreateBean createBeanInstance 调用构造函数 执行实例化 populateBean 执行依赖注入 initializeBean invokeAwareMethods 执行实现了 Aware 接口的方法 applyBeanPostProcessorsBeforeInitialization 执行 @PostConstruct invokeInitMethods afterPropertiesSet invokeCustomInitMethod 总结 : 先执行构造函数 执行依赖注入 执行 Aware 方法 执行 @PostConstruct 执行实现 InitializingBean 的 afterPropertiesSet 执行自定义的 init 方法 SourceCode Spring Aop AOP 综述 Join Point : 在 Spring Aop 中, Join Point 始终代表一个方法的执行 Advice : 在 Join point 处执行的动作, before after around 等等 Pointcut : 匹配 Join point 的表达式 Weaving : Advice 正常执行顺序 @Around 的 before @Before @Around 的 after @After (相当于finally) 用来执行释放资源等操作 @AfterReturning Advice 异常执行顺序 @Around 的 before @Before @After (相当于finally) 用来执行释放资源等操作 @AfterThrowing 开启 Spring Aop 支持 @EnableAspectJAutoProxy 此注解会加载 AnnotationAwareAspectJAutoProxyCreator , 类关系图如下 AnnotationAwareAspectJAutoProxyCreator 类图 开启步骤 &amp; AnnotationAwareAspectJAutoProxyCreator 创建时机 refresh() invokeBeanFactoryPostProcessors invokeBeanDefinitionRegistryPostProcessors AspectJAutoProxyRegistrar.registerBeanDefinitions 创建 AutoProxyCreator 这个 BeanDefinition 并注册到 BeanDefinitionMap 中 执行步骤 解析切面以及表达式 refresh() finishBeanFactoryInitialization beanFactory.preInstantiateSingletons() 当容器中第一个 Bean ( 非BeanPostProcessor 非 BeanFactoryPostProcessor) 被初始化时, 解析 Pointcut 表达式以及切面 getBean resolveBeforeInstantiation Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. AbstractAutoProxyCreator.shouldSkip findCandidateAdvisors() 寻找候选通知 this.aspectJAdvisorsBuilder.buildAspectJAdvisors() BeanFactoryUtils.beanNamesForTypeIncludingAncestors() 找出所有的 BeanName isEligibleBean isAspect 类是否被 @Aspect 标注 this.advisorFactory.getAdvisors(factory); 获取此类中所有的 Advisor (已经排序好) 解析 Pointcut 或者 Advisor 表达式 创建被拦截对象的代理 getBean 创建符合 Pointcut 表达式的, 我们自己声明的对象 doCreateBean populateBean 给bean的属性赋值, 注入依赖的对象等等 initializeBean 创建目标对象的代理并返回 如果目标对象没有实现接口, 返回 cglib 代理对象 如果目标对象实现接口, 返回 jdk 动态代理对象 执行拦截逻辑 在调用 applicationContext.getBean 获取我们的业务对象之后 , 返回的是业务对象的代理 如果是 cglib 代理对象 , 则执行 DynamicAdvisedInterceptor.intercept 方法 如果是 jdk 动态代理, 则执行 JdkDynamicAopProxy.invoke 方法 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass) 获取 aop 将要执行的拦截链(责任链设计模式) ReflectiveMethodInvocation.proceed() Advice 正常执行顺序 为什么先制性 around 的 before 而不是 @Before ​ 答: 因为在执行 around 通知时, 只有调用了 joinPoint 的 proceed() 方法, 才会把 MethodBeforeAdviceInterceptor 加入到调用栈中. doGetBean getMergedLocalBeanDefinition(beanName) getDependsOn() getSingleton beforeSingletonCreation(beanName) 暴露早期引用 singletonFactory.getObject() resolveBeforeInstantiation(beanName, mbdToUse) 初始化AOP代理,寻找切面 doCreateBean(beanName, mbdToUse, args) addSingletonFactory() 移除三级缓存 添加到二级缓存 还未给属性赋值 populateBean(beanName, mbd, instanceWrapper) 给属性赋值 initializeBean(beanName, exposedObject, mbd) 如果是AOP 这里返回代理对象 afterSingletonCreation(beanName) addSingleton(beanName, singletonObject) 移除二级缓存 添加到一级缓存 注解驱动解析 在使用注解驱动AnnotationConfigApplicationContext时,会加载以下 Bean ConfigurationClassPostProcessor BeanFactoryPostProcessor DefaultEventListenerFactory EventListenerMethodProcessor BeanFactoryPostProcessor AutowiredAnnotationBeanPostProcessor BeanPostProcessor CommonAnnotationBeanPostProcessor BeanPostProcessor 其中,起主要作用的是 ConfigurationClassPostProcessor ConfigurationClassPostProcessor 是BeanFactoryPostProcessor 类型 负责解析每一个被 @Configuration 注解标注的类, 解析的注解如下 @PropertySource @ComponentScan @Import @ImportResource @Bean AutowiredAnnotationBeanPostProcessor AutowiredAnnotationBeanPostProcessor 是 BeanPostProcessor 类型 负责处理自动装配 @Autowired @Value 在 populate 方法执行时被调用 CommonAnnotationBeanPostProcessor CommonAnnotationBeanPostProcessor 是 BeanPostProcessor 类型 负责处理一些通用自动装配 @WebServiceRef @EJB @Resource @PostConstruct @PreDestroy @Import 标签工作原理 Concept BeanPostProcessor 定义 The BeanPostProcessor interface defines callback methods that you can implement to provide your own (or override the container’s default) instantiation logic, dependency-resolution logic, and so forth. If you want to implement some custom logic after the Spring container finishes instantiating, configuring, and initializing a bean, you can plug in one or more custom BeanPostProcessor implementations. 如果想改变 Bean 的属性, 则使用 BeanPostProcessor 如果想改变 BeanDefinition, 则使用 BeanFactoryPostProcessor 比如解析${}表达式并给对象赋值 (典型: 解析配置文件) Bean 先被初始化, 然后才执行 PostProcessor 的逻辑]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>spring framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex3]]></title>
    <url>%2F2019%2F01%2F29%2Fml-ex3%2F</url>
    <content type="text"><![CDATA[lrCostFunction.m 这道题在上一个练习里已经做过,不再赘述: 123456h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta(2:end) .^ 2);theta_tmp = theta;theta_tmp(1) = 0;grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_tmp; 完成 oneVsAll.m 这道题的目的是通过fmincg算法训练出针对每一个分类(这里是1-10)所对应的\(\theta_0\)到\(\theta_n\)值,然后用这些\(\theta\)值去识别新的图片 我们的特征矩阵是5000x400,每一行代表一个图片,加上一列\(x_0\)就是5000x401,针对10个分类,我们最后要返回的all_theta是10x401的矩阵,所以我们要对所有分类(1-10)进行循环并组合成all_theta. all_theta第一行代表数字1经过训练后所对应的所有\(\theta\)值,第二行代表数字2经过训练后所对应的所有\(\theta\)值 ... 第10行代表数字0经过训练后所对应的所有\(\theta\)值 12345678for c = 1:num_labels initial_theta = zeros(n + 1, 1); options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50); [theta] = ... fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ... initial_theta, options); all_theta(c,:) = theta&apos;; endfor 完成 predictOneVsAll.m 得到了所有分类(对应1-10的数字)的\(\theta\)值后,接下来就要利用这些值去识别新的图片了 我们的\(X\)矩阵是5000x401,all_theta矩阵是10x401.很显然我们要用矩阵\(X\)与all_theta矩阵的转置相乘,再代入sigmoid函数,得到的是5000 x 10的矩阵,行数就是图片张数,第一列代表的是图片为1时的概率,第二列代表的是图片为2的概率...第十列代表的是图片为0概率 最后要做的是取每一行中概率最大的那一列就是图片最终识别结果 12h = sigmoid(X * all_theta&apos;);[max_num,p] = max(h , [] , 2); 注:max函数第3个参数传1代表求每一列的最大值,最后返回的是行向量.第3个参数传2代表求每一行的最大值,最后返回的是列向量.max函数第一个返回值代表最大的数是几,第二个参数代表这个最大数的下标 predict.m 之前的题目是用逻辑回归识别手写图片,这道题是用神经网络(Neural Network)识别手写图片 相比逻辑回归的线性classifier,NN可以执行更加复杂的任务,在这个练习中,我们要用Feedforward Propagation算法实现一个3层神经网络,算法如下图所示: 因此,代码如下: 123456789a_1 = [ones(m,1) , X];z_2 = a_1 * Theta1&apos;;a_2 = [ones(m,1),sigmoid(z_2)];z_3 = a_2 * Theta2&apos;;a_3 = sigmoid(z_3);[max_num,p] = max(a_3,[],2); 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex2]]></title>
    <url>%2F2019%2F01%2F28%2Fml-ex2%2F</url>
    <content type="text"><![CDATA[sigmoid.m 第一题比较简单,让我们实现Sigmoid函数,函数如下: \[ g(z)=\frac{1}{1+e^{-z}} \] 1g = 1 ./ (1 + exp(-z)); 完成 costFunction.m (逻辑回归的代价函数) 逻辑回归的hypothesis函数如下 \[ h_\theta(x)=g(\theta^Tx) \] 其中\(g\)就是我们刚才实现的sigmoid函数,逻辑回归的代价函数如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) \] 为什么这么复杂呢...因为这个函数可以把我们的代价函数变成非凸函数(non-convex),这样在求导的时候就不会出现local minimum 有了ex-1的基础,用octave/matlib实现也不是那么困难了,代码如下 12h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)); 逻辑回归的梯度下降与线性回归的梯度下降公式长的一样: \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 注:下标\(j\)代表第几个特征 需要注意的是,虽然它们公式长的一样,但是注意\(h_\theta(x)\)的实现方式是不同的,代码实现如下: 1grad = 1 / m * (X&apos; * (h - y)); 完成 predict.m 也是一道简单的题,直接上结果 12h = sigmoid(X * theta);p = h &gt;= 0.5; costFunctionReg.m 这道题主要是代价函数的正则化,用来解决过拟合(over-fitting)的问题,算法如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2 \] 其中\(m\)代表数据的条数,\(n\)代表特征个数,需要注意的是,正则化是不包括\(\theta_0\)的 关于向量的平方和已经在上一篇ex-1中介绍过了,因此代码实现如下: 123h = sigmoid(X * theta);theta_temp = theta(2:end);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta_temp .^ 2); 接下来就要计算梯度值了,跟计算代价函数一样,计算梯度值的时候对\(\theta_0\)不做正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 对于\(\theta_0\)之外的参数,需要加上正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j \] 因此,在计算的时候,我们可以手动的给\(\theta_0\)赋值为0,这样就可以用一个公式来计算,不用判断是否为\(\theta_0\),计算梯度的代码如下: 12theta_temp = [0;theta_temp];grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_temp; 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论读书]]></title>
    <url>%2F2019%2F01%2F28%2Fof_study%2F</url>
    <content type="text"><![CDATA[读书足以怡情，足以傅彩，足以长才。其怡情也，最见于独处幽居之时；其傅彩也，最见于高谈阔论之中；其长才也，最见于处世判事之际。练达之士虽能分别处理细事或一一判别枝节，然纵观统筹、全局策划，则舍好学深思者莫属。读书费时过多易惰，文采藻饰太盛则矫，全凭条文断事乃学究故态。读书补天然之不足，经验又补读书之不足，盖天生才干犹如自然花草，读书然后知如何修剪移接；而书中所示，如不以经验范之，则又大而无当。有一技之长者鄙读书，无知者羡读书，唯明智之士用读书，然书并不以用处告人，用书之智不在书中，而在书外，全凭观察得之。读书时不可存心诘难作者，不可尽信书上所言，亦不可只为寻章摘句，而应推敲细思。书有可浅尝者，有可吞食者，少数则须咀嚼消化。换言之，有只须读其部分者，有只须大体涉猎者，少数则须全读，读时须全神贯注，孜孜不倦。书亦可请人代读，取其所作摘要，但只限题材较次或价值不高者，否则书经提炼犹如水经蒸馏、淡而无味矣。 读书使人充实，讨论使人机智，笔记使人准确。因此不常作笔记者须记忆特强，不常讨论者须天生聪颖，不常读书者须欺世有术，始能无知而显有知。读史使人明智，读诗使人灵秀，数学使人周密，科学使人深刻，伦理学使人庄重，逻辑修辞之学使人善辩：凡有所学，皆成性格。人之才智但有滞碍，无不可读适当之书使之顺畅，一如身体百病，皆可借相宜之运动除之。滚球利睾肾，射箭利胸肺，慢步利肠胃，骑术利头脑，诸如此类。如智力不集中，可令读数学，盖演题须全神贯注，稍有分散即须重演；如不能辨异，可令读经院哲学，盖是辈皆吹毛求疵之人；如不善求同，不善以一物阐证另一物，可令读律师之案卷。如此头脑中凡有缺陷，皆有特药可医。 1注:中学时背的滚瓜烂熟的课文,过了N多年之后再来看这些又有了新解.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>论读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex1]]></title>
    <url>%2F2019%2F01%2F22%2Fml-ex1%2F</url>
    <content type="text"><![CDATA[warmUpExercise.m 第一题很简单,跟标题一样,是个热身题.要求生成一个5x5的单位矩阵 1A = eye(5); 完成 computeCost.m (计算单变量代价函数) 回想一下我们的单变量hypothesis公式 \[ h(\theta) = \theta_0 + \theta_1x \] 在这个公式中\(x\)指的是变量(variable),\(\theta_0\)和\(\theta_1\)是参数(parameter),之后的工作就是对代价函数(cost function)求导,找出使得代价函数最小(收敛时)\(\theta_0\)和\(\theta_1\)的值 刚开始学的时候有一段时间变量(variable)和参数(parameter)傻傻分不清 \(X\)是我们的数据矩阵,如下 \[ \left[ \begin{matrix} 3\\ 5\\ 8 \end{matrix} \right] \] 每一列代表一个特征,由于我们是单变量代价函数,所有X只有一列,为了不失一般性,可以改写我们的hypothesis公式 \[ h(\theta) = \theta_0x_0+ \theta_1x_1 \] 其中\(x_0\)的值始终为1,在我们的编程作业中\(\theta\)是一个向量代表\(\theta_1\)和\(\theta_2\),如下 \[ \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 为了用矩阵的乘法表示hypothesis公式,我们需要手动给\(X\)加上一列,如下(在ml编程作业中,已经帮我们添加了第一列1,所以无需我们再手动添加) \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] \] 然后执行矩阵的乘法 \[ h = X * \theta^T \] \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 代价函数 \[ J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}))^2 \] hypothesis计算完成后得到的是个m x 1的向量,代表的是我们通过h公式计算出的预测结果,m代表数据的行数,也就是\(X\)矩阵的行数.\(y\)向量代表的是真实的结果,所以\((h_\theta(x^{(i)}) - y^{(i)}))\)可以直接用两矩阵相减得出,即\(h - y\) 下面要做的就是计算平方和了,计算平方和有两种方法: \(\alpha^T * \alpha\) 永乐大帝在上,受小弟一拜 利用octave/matlib内置矩阵运算方法,先同时给所有数平方,再sum 于是,我们就可以得出我们的代价函数 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); 完成. gradientDescent.m (梯度下降) 为了使我们的线性方程方程\(h\)更贴近我们的数据集,因此我们要不断的去改变\(\theta_0\)和\(\theta_1\)这两个参数的值,使得代价函数\(J(\theta)\)取得最小值,如何取得最小值,我们用的是批量梯度下降算法(batch gradient descent algorithm) 回顾我们的批量梯度下降算法,即同时对每一个\(\theta\)求偏导,如下 \[ \theta = \theta - \alpha\cdot\frac{\partial}{\partial\theta}J(\theta) \] 即 \[ \theta_j = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j \] 由于我们是单变量,所以j只能取0,1,而且每次迭代我们必须要同时更新\(\theta_0\)和\(\theta_1\) (simultaneously update \(\theta_j\) for all j) 要计算 \((h_\theta(x^{(i)})-y^{(i)})\)的和,我们可以使用矩阵的乘法,由上文知\(X\)矩阵为m x 2 , \((h_\theta(x^{(i)})-y^{(i)})\) 为m维向量,则 \[ \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j = X^T*(h_\theta(x^{(i)})-y^{(i)}) = \left[ \begin{matrix} 1 &amp; 1 &amp; 1\\ 3 &amp; 5 &amp; 8\\ \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \\ 0 \end{matrix} \right] \] 可以简单的理解为\(X^T\)矩阵的第一行就是计算\(\theta_0\),第二行就是计算\(\theta_1\),第n行就是计算\(\theta_n\)(n &gt; 1就是多变量线性回归了,在这里我们的n = 1,也就是单变量线性回归) 因此,我们的代码可以这么写 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)); 完成 至此,我们的必修部分的编程作业都已做完,下面来看选修部分的作业 featureNormalize.m (特征标准化) 特征标准化的目的就是为了提升梯度下降的计算速度. 特征标准化包含两个方面: Feature Scaling Mean Normalize 它们将尝试使所有的特征都尽量放缩到-1到1之间(不严格,可有误差) 最简单的实现方式是\(x_n=\frac{x_n-\mu_n}{s_n}\),其中\(u_n\)是平均值,\(s_n\)是标准差 在octave中,一个mxn矩阵是可以和另一个m行的列向量或者n列的行向量相加减的,所以答案如下 123mu = mean(X);sigma = std(X);X_norm = (X_norm - mu) ./ sigma; 完成 computeCostMulti.m 由于矩阵乘法的原因,多变量线性回归的代价函数代码和单变量线性回归的代价函数代码一模一样,不再赘述 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); gradientDescentMulti.m 同理,多变量梯度下降的代码和单变量梯度下降的代码一样,不再赘述 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)) ; 完成 normalEqn.m(正规方程) 正规方程适用于特征个数不是特别多的场景(\(x&lt;10,000\)) , 优点: 不用迭代,一次即可计算出结果 不用选取学习率\(\alpha\)(learning rate) 不需要特征放缩 缺点: - 对于特征值个数\(x&gt;10,000\)时计算速度缓慢 - 只适用于线性回归模型,不适用于逻辑回归模型 - 可能出现矩阵不可逆的情况 公式: \[ (X*X^T)^{-1}X^Ty \] 由于公式本来就是用矩阵表示,所以用octave/matlib实现起来一点都不难,代码如下: 1theta = pinv(X&apos; * X) * X&apos; * y; 完成 最后,提交作业]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex1</tag>
      </tags>
  </entry>
</search>
