<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ubuntu20.04 安装图神经网络（GNN）相关包]]></title>
    <url>%2F2021%2F03%2F01%2Fubuntu20.04_install_gnn%2F</url>
    <content type="text"><![CDATA[ubuntu 20.04 安装图神经网络（GNN）相关包 前言 做好 Windows、Ubuntu 双系统之后开始安装图神经网络需要的各种包。遇到了各种问题，花了整整一天的时间才最终调试完毕。完美运行代码，没有 error 没有 warning。 安装显卡驱动（略） 查看显卡驱动是否安装成功 1nvidia-smi 安装Anaconda（略） 使用 conda 环境本来是方便包管理，但是安装 pyG 和 torch 必须只能用 pip install，不然会报错（错误内容见 pyG 安装）。 用 conda 只是图个创建环境方便，一旦使用 pip，以后安装就尽量全部使用 pip，防止包冲突。 Anaconda清华源 创建 conda 环境 12conda create -n gnn python=3.7 numpy=1.16.1conda activate gnn 注：为什么numpy版本用 1.16.1 呢？详见numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 192 from PyObject. 安装 CUDA 11.1 安装cuda(要使用runfile方式 用别的会出错) 注：使用 runfile 安装时要取消勾选显卡驱动（既不安装显卡驱动，因为它会自动卸载已安装的显卡驱动，并且安装的驱动可能是很旧的版本），安装完成后要添加环境变量。 Please make sure that - PATH includes /usr/local/cuda-11.1/bin - LD_LIBRARY_PATH includes /usr/local/cuda-11.1/lib64, or, add /usr/local/cuda-11.1/lib64 to /etc/ld.so.conf and run ldconfig as root 12export PATH="$PATH:/usr/local/cuda-11.1/bin"export LD_LIBRARY_PATH="/usr/local/cuda-11.1/lib64" 安装 CUDNN（略） 安装torch 配置pip pip清华源 安装 1pip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html 版本对应非常重要，一定要注意！！！ 版本对应非常重要，一定要注意！！！ 版本对应非常重要，一定要注意！！！ 版本对应非常重要，一定要注意！！！ 版本对应非常重要，一定要注意！！！ 安装pyG 如果采用conda安装会遇到Undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs问题 I have solved this problem. The reason is that: do not install pytorch via anaconda-navigator ! It will install both cpu and gpu versions, even though you only select the pytorch. You can download the specific version of the pytorch from: https://github.com/rusty1s/pytorch_geometric/issues/999 and this problem will be solved. All dependent packages are listed: cu101/torch-1.4.0-cp38-cp38-linux_x86_64.whl cu101/torchvision-0.5.0-cp38-cp38-linux_x86_64.whl torch_cluster-1.5.3+cu101-cp38-cp38-linux_x86_64.whl torch_scatter-2.0.4+cu101-cp38-cp38-linux_x86_64.whl torch_sparse-0.6.1+cu101-cp38-cp38-linux_x86_64.whl torch_spline_conv-1.2.0+cu101-cp38-cp38-linux_x86_64.whl 应采用pip手动安装的方式，同样注意版本号的对应关系 12345pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.htmlpip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.htmlpip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.htmlpip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu110.htmlpip install torch-geometric 安装 jupyter 1conda install -c conda-forge jupyterlab -y 安装 networkX 1conda install -c anaconda networkx -y 安装 matplotlib 1conda install -c conda-forge matplotlib -y 简单验证 1234567import torcha = torch.tensor(1.)print("cuda is available: &#123;&#125;".format(torch.cuda.is_available()))a.cuda()from torch.backends import cudnnprint("cudnn is available: &#123;&#125;".format(cudnn.is_available()))print("cudnn is acceptable: &#123;&#125;".format(cudnn.is_acceptable(a.cuda()))) 究极验证 CS224W - Colab 0.ipynb 如果上述代码下载到本地完美运行则代表完美安装成功]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>ubuntu 20.04</tag>
        <tag>图神经网络</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数与梯度的一些思考]]></title>
    <url>%2F2020%2F01%2F13%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[常见激活函数 Sigmoid 优点 : 把函数值压缩到了[0,1]范围内 have nice interpretation as a staturating &quot;firing rate&quot; of a neuron 缺点 : 梯度消失 当\(x\)的值取正负无穷大时，Sigmoid函数的导数几乎为零，这导致在反向传播应用链式法则时梯度几乎为0。 输出结果不是&quot;0中心&quot; 指数函数计算量过大 tanh 优点 : 把函数值压缩到了[-1,1]内 输出结果是&quot;0中心&quot; 缺点 : 仍然会出现梯度消失 Relu 优点 : 当 \(x &gt; 0\) 时不会出现梯度消失 计算量小，并且在实践中比sigmoid和tanh收敛速度要快很多 缺点 : 输出结果不是&quot;0中心&quot; 当\(x &lt; 0\)时，会出现梯度消失 Leakly ReLU Maxout 为什么要&quot;0中心&quot; why zero centered 参考资料 [1] Activation function [2] 详解机器学习中的梯度消失、爆炸原因及其解决方法 [3] 激活函数]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
        <tag>Activation function</tag>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈目标检测之SSD]]></title>
    <url>%2F2019%2F12%2F14%2FSSD%2F</url>
    <content type="text"><![CDATA[目标检测之SSD SSD 全称 Single Shot MultiBox Detector 是ECCV2016年的一篇文章, 是 one-stage 目标检测模型的代表作之一. 最近看了很多关于 SSD 的资料, 对 SSD 的来龙去脉也算是有点头绪, 遂打算记录下来, 另外本篇文章不涉及到具体算法, 只是从全局角度上对主要概念做下分析. 我们知道, Faster R-CNN 在当时的目标检测领域已经取得了很不错的成绩, 但是受制于模型本身的 two-stage 架构Faster R-CNN 在 VOC2007 数据集上只能达到 7fps 也就是每秒钟处理 7 张图片, 我们知道流畅的视频一般都是 24fps 以上, 因此 Faster R-CNN 还远远达不到实时处理视频的要求. 为了满足实时目标识别的要求, SSD/YOLO 横空出世, 下图是当时主流目标检测算法在 VOC2007 数据集上的表现. 可以看出在当时 SSD 力压群雄, 在 300x300 分辨率的图片下 mAP 达到了的 77% , fps 也高达 46 , 在 512x512 分辨率虽然识别速度不及 SSD300 , 但是 mAP 却达到了惊人的 80% , 这样的成绩在当年可是吊打 Faster R-CNN 和 YOLO. 值得一提的是 VOC2007 数据集上大物体较多, 对 SSD 算法比较友好, 但是在小物体识别方面, SSD 还是远远不如 Faster R-CNN 的, 至于为什么这个待会儿再说 为什么 SSD 在达到如此高的识别率时还能保证非常快的呢, 这与 SSD 的 one-stage 架构模型是分不开的,下面看一段 Faster R-CNN 伪代码 12345feature_maps = process(image)ROIs = region_proposal(feature_maps)for ROI in ROIs patch = roi_align(feature_maps, ROI) results = detector2(patch) # Reduce the amount of work here! 上面代码是典型的 two-stage , 先找到大约 2000 个候选区, 再对每个候选区的物体分别进行识别, 这是最耗时的地方. SSD 等 one-stage 算法的伪代码如下 12feature_maps = process(image)results = detector3(feature_maps) # No more separate step for ROIs SSD 架构模型 先来看看 SSD 的整体架构 是不是一脸懵逼, 没关系我也是. 我们把整个模型架构可以拆分成两部分来看, 第一部分是用 vgg16 网络来抽取特征得到特征图 (feature maps) SSD 使用 Conv4_3 这一层来抽取特征图, 为什么这么多层偏偏使用 Conv4_3 呢, 来看下图就能明白 我们知道不同维度的特征图所抽取得到的图像特征是不同的, 某些特征图可能代表边界, 有些代表纹理, 而 vgg16 的 Conv4_3 这一层就代表物体特征, 在使用 Conv4_3 后, SSD 又额外添加了 6 层辅助卷积层, 目的是为了识别不同大小的目标, 至于为什么稍后再做说明 在使用 vgg16 得到物体特征图后, SSD 额外添加了 默认边界框 Default boundary box 注: default box 跟 priors box 其实是一回事 默认边界框的概念与 Faster R-CNN 中的锚点概念相似, 就是预设一些目标预选框和 ground truth 进行对比, 而且默认边界框是人为设定的, 也就意味着程序不用经过复杂的算法去生成默认边界框, 这大大减少了计算量, 而且默认边界框的大小的选择非常有讲究. 为什么说是有讲究, 因为 SSD 选择的 4 个或 6 个默认边界框的长宽比例几乎囊括了生活中上常见物体的形状, SSD 在不同的特征图上有不同个数的默认边界框, 但也只可能是 4 个或 6 个. 至于为什么 SSD 选择了(4 , 6) 是因为在识别速度和识别精度之间做了取舍, 默认边界框多了, 识别精度会提高, 但是识别速度会下降, 反之亦然 关于默认边界框的选取的详细内容可以参考Generating Anchor boxes for Yolo-like network for vehicle detection using KITTI dataset 在特征图每个单元(或者也可以简单的理解为矩阵的每个点)上都会生成 4 个或 6 个默认边界框, 所有特征图加起来一共会生成 8732 个默认边界框 (注: 8732 是 300x300 分辨率的图像, 512x512 分辨率的图像则是 24564 个默认边界框) 默认边界框匹配策略 如果在某一个固定的特征单元上, 某个默认边界框和 ground truth 的 IOU 大于 0.5 则把这个默认边界框视为正匹配, 否则就是负匹配. (IoU 即 the intersection over the union, 是两个图像的交集比并集, 又称为 Jaccard 相似系数) 为了简单起见, 我们把特征图看做 8x8 (实际是 38x38 或者其他) 并且只有 3 个默认候选框 在第 5 行第 4 列这个特征单元上, 蓝色代表 ground truth , 绿色代表默认边界框, 其中 1, 2 两个默认候选框的 IoU 大于 0.5 则 1, 2 两个默认候选框就是正匹配, 这样我们就可以得到目标的大致形状. 多尺度特征图 上图源自 SSD 论文, 论文中也是假设有一个 8x8 特征图一个 4x4 特征图, 以及 3 个默认边界框 可以看到在 8x8 特征图上, 狗狗的所有默认边界框与 ground truth 的 IoU 小于 0.5, 因此在 8x8 特征图上, 狗狗不会被识别出来, 但是在 8x8 特征图上, 猫的 3 个默认边界框有 2 个 IoU 大于 0.5 (蓝色框框), 因此猫在图片中的的形状被大致确认下来. 反而在 4x4 特征图上, 狗狗的一个默认边界框与 ground truth 的 IoU 大于 0.5 (红色框框) , 但是猫的 IoU 都小于 0.5, 因此猫在 4x4 特征图上就没有被识别出来 这就是多尺度特征图, 对于 8x8 特征图我们叫做高分辨率特征图 (higher resolution feature maps) , 反之亦然 高分辨率特征图非常适合识别小物体, 低分辨率特征图非常适合识别大物体 SSD 总共有 6 个尺度的特征图参与目标识别, 这就保证了对不同尺寸的物体都可以有较好的识别表现. 值得注意的是输入图像的分辨率是 300x300 , 但是我们第一个特征图是 38x38, 从 300 到 38 是个非常大的比例了, 从而造成 SSD 对小目标的识别效果不是很好 总结 下面用一张图来总结 SSD 在前 3 个特征图上, 由于特征图分辨率高, 并没有识别出汽车, 紧接着最后 3 个特征图都识别出了汽车, 并且一共生成了 4 个边界框, 最后再经过非极大抑制最终确定一个边界框, 至此识别完成. ps : 目前刚入门目标检测没多久, 知识储备还较少, 还是只菜狗, 这篇文章就先这样, 等以后知识丰富了回过头来再改 参考资料 [1] SSD PPT [2] SSD详解 [3] Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning [4] SSD object detection: Single Shot MultiBox Detector for real-time processing [5] What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>Single Shot MultiBox Detector</tag>
        <tag>目标检测</tag>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 总结]]></title>
    <url>%2F2019%2F08%2F15%2Fjvm%2F</url>
    <content type="text"><![CDATA[JVM 知识整理 Q &amp; A 为什么会 STW 根据不同的垃圾收集器 STW 的时机也不同, 大体上可以分为 3 种 MARK - 标记阶段必须要暂停用户线程, 防止用户线程改变对象的引用. SWEEP - 清除阶段, 扫描没有被标记的内存 COMPACT - 整理阶段, 给对象重新分配内存来减少内存碎片 注: CMS 收集器是不会进行内存整理的 (Compact), 但是当要进入老年代的对象大小 &gt; max{内存碎片}时, Serial Old GC 会进行碎片整理, 当然这需要花费很多时间. 参考why STW CMS 收集器什么时候可能触发 STW 的 FULL GC Perm 空间不足 CMS GC 时出现 promotion failed 和 concurrent mode failure（concurrent mode failure 发生的原因一般是CMS正在进行，但是由于老年代空间不足，需要尽快回收老年代里面的不再被使用的对象，这时停止所有的线程，同时终止 CMS，直接进行Serial Old GC） 统计得到的 Young GC 晋升到老年代的平均大小大于老年代的剩余空间 主动触发Full GC（执行jmap -histo:live [pid]）来避免碎片问题 如果新生代内存不够, 会出现什么情况 会触发一次 Minor GC 如果新生代 GC 后内存依然不够, 会出现什么情况 When allocation inside a TLAB is not possible (typically because there’s not enough room there), the allocation moves on to a shared Eden space. If there’s not enough room in there either, a garbage collection process in Young Generation is triggered to free up more space. If the garbage collection also does not result in sufficient free memory inside Eden, then the object is allocated in the Old Generation. 会分配到老年代 Eden 区执行完垃圾回收之后会怎样 After the marking phase is completed, all the live objects in Eden are copied to one of the Survivor spaces. The whole Eden is now considered to be empty and can be reused to allocate more objects. Such an approach is called “Mark and Copy”: the live objects are marked, and then copied (not moved) to a survivor space. Eden区执行完垃圾回收后, 被标记为存活的对象会被 移动! 移动! 移动! 到 Survivor 区, 之后 Eden 区会被视作为空 Survivor 区的对象是如何到老年代的 每一次 GC 后依然存活的对象年龄会 +1 , Survivor 区年龄到15的对象会直接进入老年代 -XX:+MaxTenuringThreshold 来设置年龄 一些特殊情况下, 会让对象被视作足够老, 从而被移动到老年代 Survivor 区没有足够的空间去容纳所有年轻代的存活对象 动态年龄调整, 当 Survivor 区中相同年龄的对象大小总和 &gt; Survivor 内存的一半时, 将直接进入老年代, 并且年龄阈值也会随之更改 CMS 收集器的标记清除过程 初始标记 (Init-mark) 并发标记 (Concurrent-mark) 重新标记 (Remark) 常见的垃圾收集器有哪些 Java 有4种垃圾收集器(jdk10 之前), JDK11 采用 ZGC Serial Garbage Collector S GC Parallel Garbage Collector P GC CMS Garbage Collector CMS GC G1 Garbage Collector G1GC The Z Garbage Collector ZGC 默认的垃圾收集器 Java 7 - Parallel GC Java 8 - Parallel GC Java 9 - G1 GC Java 10 - G1 GC Java 11 - ZGC 基础知识 常见虚拟机 Hotspot J9 Hotspot 垃圾回收算法基础 垃圾回收算法是主要基于两点 找到所有存活对象 清理没有被使用的对象 如何找到存活对象呢? 引用计数器 难以解决对象循环引用问题 对象可达性分析 要实现可达性分析, 必须要规定一些起始点, 这些起始点叫做 GC Roots 4种 GC Roots 局部变量 活跃线程 java 本地方法 静态变量 通过 GC Roots 找到存活对象之后, 给对象标记为存活, 则堆中剩余其他对象则被视为垃圾 需要额外注意的点有 标记阶段为了防止引用被应用所改变, 需要暂停所有用户线程, 也叫作 Stop The World 标记阶段 STW 所消耗的时间既跟堆大小没关系, 又跟堆中的对象数量没关系. 唯一决定 STW 时间的是堆中存活对象的多少 存活对象标记完成后, 进入清理阶段, 几种常见的清理算法有 标记 - 复制 (Mark - Copy) 优点 : 不存在内存碎片 缺点 : 需要双倍内存用于复制存活对象 标记 - 删除 (Mark - Sweep) 优点 : 占用内存较小 缺点 : 存在大量内存碎片 标记 - 整理 (Mark - Sweep - Compact) 优点 : 占用内存小, 不存在内存碎片 缺点 : 整理时需要重新分配内存, 消耗大量时间 Hotspot 虚拟机内存 在 Hotspot 虚拟机中, 堆内存被划分为了3大部分 新生代 (Young Gen) Eden 所有新生成的对象都会被分配到这个区域中 Survivor 两个区内存空间一样 同一时间内两个区一定有一个为空 当 Young Gen 触发 GC 时, Eden 区的存活对象和 Survivor from 中的存活对象都会被复制到 Survivor to 区当中 老年代 (Old / Tenured Gen) 永久代 (Perm Gen 永久代在 JDK8 之后被 MetaSpace 取代) Java Heap Memory 为什么这么划分, 是根据现实场景中对象的生命周期分布来决定的, 大部分对象都是&quot;朝生夕死&quot;. 一般情况下, 经历过数次 GC 依然存活的对象会被分配到老年代中(有例外), 而且不同分代上对应的垃圾回收器也不同. Hotspot 虚拟机常见收集器 Serial G1 Parallel CMS(只用于老年代垃圾回收) 关于栈帧 每个栈帧入栈出站都代表一次方法从开始到结束的过程 只有位于栈顶的栈帧才是有效的, 这个栈帧叫做当前方法 栈帧由以下元素组成 局部变量表 操作数栈 动态链接 返回地址 .... 局部变量表 基本单位为 Slot ,虚拟机规范并没有明确规定 Slot 大小 必须能存的下 boolean byte char short int float reference returnAddress 类型的数据 long 与 double 类型被分割存储 局部变量表的大小在编译时就被完全确定 对于非静态方法, Slot 的第0位指向的是对象本身, 也就是 this 操作数栈 编译时确定最大深度 算术运算是取操作数栈的前两位, 运算结果再入栈 也可以用来调用其他方法时传递参数 动态连接 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用 一些 blog 引用 What are GC roots for classes? Why does the JVM full GC need to stop-the-world? HotSpot Virtual Machine Garbage Collection Tuning Guide ZGC Card Marking 解决老年代引用新生代对象 从实际案例聊聊Java应用的GC优化 Java bytecode]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>spring framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC 总结]]></title>
    <url>%2F2019%2F08%2F14%2Fspring-mvc%2F</url>
    <content type="text"><![CDATA[Spring MVC 总结 Q&amp;A 如何理解 Spring MVC 的父子容器 Spring 官方文档的一张图就很好诠释了父子容器 The root WebApplicationContext typically contains infrastructure beans such as data repositories and business services that need to be shared across multiple Servlet instances. Those beans are effectively inherited and could be overridden (i.e. re-declared) in the Servlet-specific, child WebApplicationContext which typically contains beans local to the given Servlet 由此可见, 子容器是可以继承父容器的所有 Bean 的. 在 web.xml 中, ContextLoaderListener 会初始化父容器 创建 ConfigurableWebApplicationContext configureAndRefreshWebApplicationContext 喜闻乐见的 refresh 方法, 开始创建IOC容器 父容器创建完成之后, 创建 DispatcherServlet 执行Servlet的 init 执行 initServletBean initWebApplicationContext setParent configureAndRefreshWebApplicationContext 喜闻乐见的 refresh 方法, 开始创建IOC容器 只有子容器, 没有父容器可以吗 当然可以 Spring MVC 如何实现注解自动装配 要实现自动装配, 必须借助于 Servlet 的 SPI SPI 要求我们必须在 META-INF.services 包下指定实现 ServletContainerInitializer 接口的类 spring-web 包就已经提供了这个实现叫做 SpringServletContainerInitializer 阅读 @EnableWebMvc 源码可知, Spring 已经给我们默认提供了几个 WebApplicationInitialize 实现 AbstractContextLoaderInitializer AbstractDispatcherServletInitializer AbstractAnnotationConfigDispatcherServletInitializer 我们只需要实现 AbstractAnnotationConfigDispatcherServletInitializer 即可 指定 RootConfigClass 也就是父容器(可不指定) 指定 ServletConfigClass 指定拦截路径 有了自动装配, 我们就可以使用 java 编程的方式实现 web 开发, 不需要任何的 xml 文件(包括 web.xml) Spring Boot 如何实现自动装配 前端控制器装配 : DispatcherServletAutoConfiguration 配置装配 : WebMvcAutoConfiguration 替换 @EnableWebMvc Servlet 容器装配 : ServletWebServerFactoryAutoConfiguration 基础知识 Spring Web MVC 架构 前端控制器模式 MVC 核心组件 Bean 类型 作用 HandlerMapping 映射请求（Request）到处理器（Handler）加上其关联的拦截器（HandlerInterceptor）列表，其映射关系基于不同的 HandlerMapping 实现的一些标准细节。其中两种主要 HandlerMapping 实现， RequestMappingHandlerMapping支持标注 @RequestMapping 的方法， SimpleUrlHandlerMapping 维护精确的URI路径与处理器的映射 HandlerAdapter 帮助 DispatcherServlet 调用请求处理器（Handler），无需关注其中实际的调用细节。比如，调用注解实现的 Controller 需要解析其关联的注解. HandlerAdapter的主要目的是为了屏蔽与 DispatcherServlet 之间的诸多细节。 HandlerExceptionResolver 解析异常，可能策略是将异常处理映射到其他处理器（Handlers） 、或到某个 HTML错误页面，或者其他。 ViewResolver 从处理器（Handler）返回字符类型的逻辑视图名称解析出实际的 View 对象，该对象将渲染后的内容输出到HTTP 响应中。 LocaleResolver, LocaleContextResolver 从客户端解析出 Locale ，为其实现国际化视图。 MultipartResolver 解析多部分请求（如 Web 浏览器文件上传）的抽象实现 MVC 交互流程 注解驱动实现 Spring Web MVC 注解配置: @Configuration 组件激活: @EnableWebMvc 注册 RequestMappingHandlerMapping 注册 RequestMappingHandlerAdapter 自定义组件: WebMvcConfigurer 添加拦截器 配置内容协商器 配置异步支持 等等... REST 内容协商 内容协商其实不仅仅包含 REST , 还有视图协商 , 由于工作中已经不再使用视图解析器, 所以主要来研究 REST 内容协商. 组件名称 实现 说明 内容协商管理器 ContentNegotiationManager ContentNegotiationStrategy 控制策略 媒体类型 MediaType HTTP 消息媒体类型, 如 text/html 消费媒体类型 @RequestMapping#consumes 请求头 Content-type 媒体类型映射 生产媒体类型 @RequestMapping#produces 响应头 Content-type 媒体类型映射 HTTP 消息转换器 HttpMessageConverter HTTP 消息转换器, 用于反序列化 HTTP 请求或序列化响应配置 REST 相关的组件 Web MVC 配置器 WebMvcConfigurer 配置 REST 的相关组件 处理方法 HandlerMethod @RequestMapping 标注的方法 处理方法参数解析器 HandlerMethodArgumentResolver 用于 HTTP 请求中解析 HandlerMethod 参数内容 处理方法返回值解析器 HandlerMethodReturnValueHandler 用于 HandlerMethod 返回值解析为 HTTP 相应内容 知道了内容协商的核心组件后 , 再来看看 Spring MVC 的 REST 处理流程 Spring 异常处理机制 官方文档中说 DispatcherServlet 会委托给一个 HandlerExceptionResolver 链去处理异常 Spring MVC 自带的异常处理器有如下几种 HandlerExceptionResolver Description SimpleMappingExceptionResolver A mapping between exception class names and error view names. Useful for rendering error pages in a browser application. DefaultHandlerExceptionResolver Resolves exceptions raised by Spring MVC and maps them to HTTP status codes. Also see alternative ResponseEntityExceptionHandler and REST API exceptions. ResponseStatusExceptionResolver Resolves exceptions with the @ResponseStatus annotation and maps them to HTTP status codes based on the value in the annotation. ExceptionHandlerExceptionResolver Resolves exceptions by invoking an @ExceptionHandler method in an @Controller or a @ControllerAdvice class. See [@ExceptionHandler methods](https://docs.spring.io/spring/docs/5.0.15.RELEASE/spring-framework-reference/web.html#mvc-ann-exceptionhandler). 需要注意的点: 只需声明我们自定义的 HandlerExceptionResolver 实现为 Spring 组件, 就会被 Spring 添加到异常处理链中 HandlerExceptionResolver 返回的 ModelAndView 即为错误页面 如果异常处理完成且不希望中断异常执行链, 则返回 null 如果异常处理完成且希望中断异常执行链, 则返回空的 ModelAndView 即可 可以实现 Ordered 接口来指定 HandlerExceptionResolver 的调用顺序 Spring MVC 常用注解 注解 用途 @RequestParam 从请求中获取参数值 @RequestHeader 获取请求头 @RequestHeader(&quot;Accept-Encoding&quot;)@RequestHeader(&quot;Keep-Alive&quot;) @CookieValue 获取 Cookie 中的值 @CookieValue(&quot;JSESSIONID&quot;) @ModelAttribute 从 request 中获取参数值, 并赋值到对应的对象中 @RequestBody 从 request 中通过 HttpMessageConverter 反序列化到对象中 @ResponseBody 把返回对象通过 HttpMessageConverter 序列化到 response 中 @ExceptionHandler 处理自定义异常写在 Controller 里则只处理本 Controller 里的异常如果想要处理全局异常, 则需要配合 @ControllerAdvice 并标注在类上注意全局异常在本地异常方法执行后才执行 @ControllerAdvice 支持 @ExceptionHandler @InitBinder @ModelAttribute 的全局处理官方不建议随意使用, 会显著的影响性能 @InitBinder 需配合 @Controller 或者 @ControllerAdvice 使用目的是为了自定义类型转换 @PathVariable 用来解析 REST 请求中的变量 @CrossOrigin 跨域支持可以标注在 Controller 方法和类上如果想实现更加精细的配置, 在 WebMvcConfigurer 中重写 addCorsMappings]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>spring framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Core 总结]]></title>
    <url>%2F2019%2F08%2F14%2Fspring%2F</url>
    <content type="text"><![CDATA[Spring Core 个人总结 Q&amp;A Spring Framework BeanFactory 和 ApplicationContext 的区别 Feature BeanFactory ApplicationContext Bean instantiation/wiring Yes Yes Integrated lifecycle management No Yes Automatic BeanPostProcessorregistration No Yes Automatic BeanFactoryPostProcessorregistration No Yes Convenient MessageSource access (for internalization) No Yes Built-in ApplicationEvent publication mechanism No Yes 简而言之, BeanFactory 只是加载了需要注入的 bean 而已,并没有注册后置处理器以及其他消息和事件管理 BeanFactory 和 FactoryBean 的区别 FactoryBean定义 : Interface to be implemented by objects used within a BeanFactory which are themselves factories for individual objects. If a bean implements thisinterface, it is used as a factory for an object to expose, not directly as abean instance that will be exposed itself. BeanFactory定义 : The root interface for accessing a Spring bean container. This is the basic client view of a bean container; 二者其实没有什么联系,BeanFactory代表的是Spring的IOC容器. 而FactoryBean则是工厂模式的实现,并且以Spring命名风格命名 ,比如UserFactory,在Spring里就叫做UserFactoryBean. Spring IOC容器如何解决循环依赖 使用三级缓存解决循环依赖 (按照存放的先后顺序) singletonFactories earlySingletonObjects singletonObjects 1234567public class A&#123; private B b;&#125;public class B&#123; private A a;&#125; createBeanInstance(beanName, mbd, args) 首先,调用A的构造器初始化A addSingletonFactory 把还未初始化变量的A放入singletonFactories中 populateBean(beanName, mbd, instanceWrapper) 给A对象的字段赋值 发现A中需要注入B getBean() 初始化B createBeanInstance(beanName, mbd, args) 调用B的构造器初始化B addSingletonFactory 把B放入singletonFactories中 populateBean(beanName, mbd, instanceWrapper) 发现B中需要注入A getBean() 去找A 发现A在singletonFactories 中 ,直接返回A 哪些循环依赖是不能被解决的 构造器中的循环依赖是不能被解决的 因为 Spring 会调用构造器初始化 Bean, 然后才会把 bean 实例放入解决循环依赖的缓存中. 没有了缓存, 执行创建逻辑就会陷入死循环. 为什么 getBean 的时候要用三级缓存, 二级缓存就解决不了吗 实际上 singletonFactories 和 earlySingletonObjects 都是暴露bean的早期引用的缓存, 这两者从宏观角度上是可以合二为一的, 但是 Spring 预留了可扩展性, 在 AbstractAutowireCapableBeanFactory 的doCreateBean 方法中,留意如下代码 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); 进入 getEarlyBeanReference 方法中,我们发现对于实现了 SmartInstantiationAwareBeanPostProcessor 接口的 Bean, 会进一步的对bean进行封装处理(比如返回 bean 的代理对象) 又根据单一职责的设计原则, 早期引用缓存这才被划分为了singletonFactories 和 earlySingletonObjects @Lazy 是如何实现的 ​ 在解析被 @Lazy 标记的 Bean 时, 会给相应的 BeanDefinition 对象里 lazy 属性设置为 true , 因而在执行初始化单例对象时, 会忽略掉被 @Lazy 标注的 Bean. ​ 在真正的用到被 @Lazy 标注的 Bean 时, 会直接调用 getBean 执行创建逻辑. ​ 当 @Lazy 标记的 Bean 被其他组件引用时, @Lazy 失效 SpringMVC 是如何解析方法中的注解,并赋值的 实现 HandlerMethodArgumentResolver @PostConstruct 是如何实现的 CommonAnnotationBeanPostProcessor 是 InitDestroyAnnotationBeanPostProcessor 的子类 doCreateBean initializeBean InitDestroyAnnotationBeanPostProcessor.applyBeanPostProcessorsBeforeInitialization invokeInitMethods Bean 的初始化赋值顺序 doCreateBean createBeanInstance 调用构造函数 执行实例化 populateBean 执行依赖注入 initializeBean invokeAwareMethods 执行实现了 Aware 接口的方法 applyBeanPostProcessorsBeforeInitialization 执行 @PostConstruct invokeInitMethods afterPropertiesSet invokeCustomInitMethod 总结 : 先执行构造函数 执行依赖注入 执行 Aware 方法 执行 @PostConstruct 执行实现 InitializingBean 的 afterPropertiesSet 执行自定义的 init 方法 同时被事务和 AOP 标记的方法, 执行顺序是怎样 如果 AOP 的 Aespct 不实现 Order 接口指定顺序, 那么在执行链中是在事务执行链之后, 反之亦然 @PostConstuct 是如何实现的 在 populateBean 之后 initializeBean 时 在 invokeAwareMethods 之后 invokeInitMethods 之前 执行 beforeInitialization 由 CommonAnnotationBeanPostProcessor 解析 @PostConstuct 注解, 找到被标记的方法 反射调用 SourceCode Spring Aop AOP 综述 Join Point : 在 Spring Aop 中, Join Point 始终代表一个方法的执行 Advice : 在 Join point 处执行的动作, before after around 等等 Pointcut : 匹配 Join point 的表达式 Weaving : 织入 Advice 正常执行顺序 @Around 的 before @Before @Around 的 after @After (相当于finally) 用来执行释放资源等操作 @AfterReturning Advice 异常执行顺序 @Around 的 before @Before @After (相当于finally) 用来执行释放资源等操作 @AfterThrowing 开启 Spring Aop 支持 @EnableAspectJAutoProxy 此注解会加载 AnnotationAwareAspectJAutoProxyCreator , 类关系图如下 AnnotationAwareAspectJAutoProxyCreator 类图 开启步骤 &amp; AnnotationAwareAspectJAutoProxyCreator 创建时机 refresh() invokeBeanFactoryPostProcessors invokeBeanDefinitionRegistryPostProcessors AspectJAutoProxyRegistrar.registerBeanDefinitions 创建 AutoProxyCreator 这个 BeanDefinition 并注册到 BeanDefinitionMap 中 执行步骤 解析切面以及表达式 refresh() finishBeanFactoryInitialization beanFactory.preInstantiateSingletons() 当容器中第一个 Bean ( 非BeanPostProcessor 非 BeanFactoryPostProcessor) 被初始化时, 解析 Pointcut 表达式以及切面 getBean resolveBeforeInstantiation Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. AbstractAutoProxyCreator.shouldSkip findCandidateAdvisors() 寻找候选通知 this.aspectJAdvisorsBuilder.buildAspectJAdvisors() BeanFactoryUtils.beanNamesForTypeIncludingAncestors() 找出所有的 BeanName isEligibleBean isAspect 类是否被 @Aspect 标注 this.advisorFactory.getAdvisors(factory); 获取此类中所有的 Advisor (已经排序好) 解析 Pointcut 或者 Advisor 表达式 创建被拦截对象的代理 getBean 创建符合 Pointcut 表达式的, 我们自己声明的对象 doCreateBean populateBean 给bean的属性赋值, 注入依赖的对象等等 initializeBean 创建目标对象的代理并返回 如果目标对象没有实现接口, 返回 cglib 代理对象 如果目标对象实现接口, 返回 jdk 动态代理对象 执行拦截逻辑 在调用 applicationContext.getBean 获取我们的业务对象之后 , 返回的是业务对象的代理 如果是 cglib 代理对象 , 则执行 DynamicAdvisedInterceptor.intercept 方法 如果是 jdk 动态代理, 则执行 JdkDynamicAopProxy.invoke 方法 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass) 获取 aop 将要执行的拦截链(责任链设计模式) ReflectiveMethodInvocation.proceed() Advice 正常执行顺序 为什么先制性 around 的 before 而不是 @Before ​ 答: 因为在执行 around 通知时, 只有调用了 joinPoint 的 proceed() 方法, 才会把 MethodBeforeAdviceInterceptor 加入到调用栈中. doGetBean getMergedLocalBeanDefinition(beanName) getDependsOn() getSingleton beforeSingletonCreation(beanName) singletonFactory.getObject() resolveBeforeInstantiation(beanName, mbdToUse) 初始化AOP代理,寻找切面 doCreateBean(beanName, mbdToUse, args) addSingletonFactory() 添加到三级缓存 此时还未给属性赋值 populateBean(beanName, mbd, instanceWrapper) 给属性赋值 initializeBean(beanName, exposedObject, mbd) 如果是AOP 这里返回代理对象 afterSingletonCreation(beanName) addSingleton(beanName, singletonObject) 移除二级缓存 添加到一级缓存 注解驱动解析 在使用注解驱动AnnotationConfigApplicationContext时,会加载以下 Bean ConfigurationClassPostProcessor BeanFactoryPostProcessor DefaultEventListenerFactory EventListenerMethodProcessor BeanFactoryPostProcessor AutowiredAnnotationBeanPostProcessor BeanPostProcessor CommonAnnotationBeanPostProcessor BeanPostProcessor 其中,起主要作用的是 ConfigurationClassPostProcessor ConfigurationClassPostProcessor 是BeanFactoryPostProcessor 类型 负责解析每一个被 @Configuration 注解标注的类, 解析的注解如下 @PropertySource @ComponentScan @Import @ImportResource @Bean AutowiredAnnotationBeanPostProcessor AutowiredAnnotationBeanPostProcessor 是 BeanPostProcessor 类型 负责处理自动装配 @Autowired @Value 在 populate 方法执行时被调用 CommonAnnotationBeanPostProcessor CommonAnnotationBeanPostProcessor 是 BeanPostProcessor 类型 负责处理一些通用自动装配 @WebServiceRef @EJB @Resource @PostConstruct @PreDestroy @Import 标签工作原理 Concept BeanPostProcessor 定义 The BeanPostProcessor interface defines callback methods that you can implement to provide your own (or override the container’s default) instantiation logic, dependency-resolution logic, and so forth. If you want to implement some custom logic after the Spring container finishes instantiating, configuring, and initializing a bean, you can plug in one or more custom BeanPostProcessor implementations. 如果想改变 Bean 的属性, 则使用 BeanPostProcessor 如果想改变 BeanDefinition, 则使用 BeanFactoryPostProcessor 比如解析${}表达式并给对象赋值 (典型: 解析配置文件) Bean 先被初始化, 然后才执行 PostProcessor 的逻辑]]></content>
      <categories>
        <category>复习</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>spring framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex3]]></title>
    <url>%2F2019%2F01%2F29%2Fml-ex3%2F</url>
    <content type="text"><![CDATA[lrCostFunction.m 这道题在上一个练习里已经做过,不再赘述: 123456h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta(2:end) .^ 2);theta_tmp = theta;theta_tmp(1) = 0;grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_tmp; 完成 oneVsAll.m 这道题的目的是通过fmincg算法训练出针对每一个分类(这里是1-10)所对应的\(\theta_0\)到\(\theta_n\)值,然后用这些\(\theta\)值去识别新的图片 我们的特征矩阵是5000x400,每一行代表一个图片,加上一列\(x_0\)就是5000x401,针对10个分类,我们最后要返回的all_theta是10x401的矩阵,所以我们要对所有分类(1-10)进行循环并组合成all_theta. all_theta第一行代表数字1经过训练后所对应的所有\(\theta\)值,第二行代表数字2经过训练后所对应的所有\(\theta\)值 ... 第10行代表数字0经过训练后所对应的所有\(\theta\)值 12345678for c = 1:num_labels initial_theta = zeros(n + 1, 1); options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50); [theta] = ... fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ... initial_theta, options); all_theta(c,:) = theta&apos;; endfor 完成 predictOneVsAll.m 得到了所有分类(对应1-10的数字)的\(\theta\)值后,接下来就要利用这些值去识别新的图片了 我们的\(X\)矩阵是5000x401,all_theta矩阵是10x401.很显然我们要用矩阵\(X\)与all_theta矩阵的转置相乘,再代入sigmoid函数,得到的是5000 x 10的矩阵,行数就是图片张数,第一列代表的是图片为1时的概率,第二列代表的是图片为2的概率...第十列代表的是图片为0概率 最后要做的是取每一行中概率最大的那一列就是图片最终识别结果 12h = sigmoid(X * all_theta&apos;);[max_num,p] = max(h , [] , 2); 注:max函数第3个参数传1代表求每一列的最大值,最后返回的是行向量.第3个参数传2代表求每一行的最大值,最后返回的是列向量.max函数第一个返回值代表最大的数是几,第二个参数代表这个最大数的下标 predict.m 之前的题目是用逻辑回归识别手写图片,这道题是用神经网络(Neural Network)识别手写图片 相比逻辑回归的线性classifier,NN可以执行更加复杂的任务,在这个练习中,我们要用Feedforward Propagation算法实现一个3层神经网络,算法如下图所示: 因此,代码如下: 123456789a_1 = [ones(m,1) , X];z_2 = a_1 * Theta1&apos;;a_2 = [ones(m,1),sigmoid(z_2)];z_3 = a_2 * Theta2&apos;;a_3 = sigmoid(z_3);[max_num,p] = max(a_3,[],2); 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex2]]></title>
    <url>%2F2019%2F01%2F28%2Fml-ex2%2F</url>
    <content type="text"><![CDATA[sigmoid.m 第一题比较简单,让我们实现Sigmoid函数,函数如下: \[ g(z)=\frac{1}{1+e^{-z}} \] 1g = 1 ./ (1 + exp(-z)); 完成 costFunction.m (逻辑回归的代价函数) 逻辑回归的hypothesis函数如下 \[ h_\theta(x)=g(\theta^Tx) \] 其中\(g\)就是我们刚才实现的sigmoid函数,逻辑回归的代价函数如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) \] 为什么这么复杂呢...因为这个函数可以把我们的代价函数变成非凸函数(non-convex),这样在求导的时候就不会出现local minimum 有了ex-1的基础,用octave/matlib实现也不是那么困难了,代码如下 12h = sigmoid(X * theta);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)); 逻辑回归的梯度下降与线性回归的梯度下降公式长的一样: \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 注:下标\(j\)代表第几个特征 需要注意的是,虽然它们公式长的一样,但是注意\(h_\theta(x)\)的实现方式是不同的,代码实现如下: 1grad = 1 / m * (X&apos; * (h - y)); 完成 predict.m 也是一道简单的题,直接上结果 12h = sigmoid(X * theta);p = h &gt;= 0.5; costFunctionReg.m 这道题主要是代价函数的正则化,用来解决过拟合(over-fitting)的问题,算法如下: \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2 \] 其中\(m\)代表数据的条数,\(n\)代表特征个数,需要注意的是,正则化是不包括\(\theta_0\)的 关于向量的平方和已经在上一篇ex-1中介绍过了,因此代码实现如下: 123h = sigmoid(X * theta);theta_temp = theta(2:end);J = -1 / m * (y&apos; * log(h) + (1 - y)&apos; * log(1 - h)) + lambda / (2 * m) * sum(theta_temp .^ 2); 接下来就要计算梯度值了,跟计算代价函数一样,计算梯度值的时候对\(\theta_0\)不做正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \] 对于\(\theta_0\)之外的参数,需要加上正则化处理 \[ \frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j \] 因此,在计算的时候,我们可以手动的给\(\theta_0\)赋值为0,这样就可以用一个公式来计算,不用判断是否为\(\theta_0\),计算梯度的代码如下: 12theta_temp = [0;theta_temp];grad = 1 / m * (X&apos; * (h - y)) + lambda / m * theta_temp; 完成]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论读书]]></title>
    <url>%2F2019%2F01%2F28%2Fof_study%2F</url>
    <content type="text"><![CDATA[读书足以怡情，足以傅彩，足以长才。其怡情也，最见于独处幽居之时；其傅彩也，最见于高谈阔论之中；其长才也，最见于处世判事之际。练达之士虽能分别处理细事或一一判别枝节，然纵观统筹、全局策划，则舍好学深思者莫属。读书费时过多易惰，文采藻饰太盛则矫，全凭条文断事乃学究故态。读书补天然之不足，经验又补读书之不足，盖天生才干犹如自然花草，读书然后知如何修剪移接；而书中所示，如不以经验范之，则又大而无当。有一技之长者鄙读书，无知者羡读书，唯明智之士用读书，然书并不以用处告人，用书之智不在书中，而在书外，全凭观察得之。读书时不可存心诘难作者，不可尽信书上所言，亦不可只为寻章摘句，而应推敲细思。书有可浅尝者，有可吞食者，少数则须咀嚼消化。换言之，有只须读其部分者，有只须大体涉猎者，少数则须全读，读时须全神贯注，孜孜不倦。书亦可请人代读，取其所作摘要，但只限题材较次或价值不高者，否则书经提炼犹如水经蒸馏、淡而无味矣。 读书使人充实，讨论使人机智，笔记使人准确。因此不常作笔记者须记忆特强，不常讨论者须天生聪颖，不常读书者须欺世有术，始能无知而显有知。读史使人明智，读诗使人灵秀，数学使人周密，科学使人深刻，伦理学使人庄重，逻辑修辞之学使人善辩：凡有所学，皆成性格。人之才智但有滞碍，无不可读适当之书使之顺畅，一如身体百病，皆可借相宜之运动除之。滚球利睾肾，射箭利胸肺，慢步利肠胃，骑术利头脑，诸如此类。如智力不集中，可令读数学，盖演题须全神贯注，稍有分散即须重演；如不能辨异，可令读经院哲学，盖是辈皆吹毛求疵之人；如不善求同，不善以一物阐证另一物，可令读律师之案卷。如此头脑中凡有缺陷，皆有特药可医。 1注:中学时背的滚瓜烂熟的课文,过了N多年之后再来看这些又有了新解.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>论读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达ml课程编程作业 machine-learning-ex1]]></title>
    <url>%2F2019%2F01%2F22%2Fml-ex1%2F</url>
    <content type="text"><![CDATA[warmUpExercise.m 第一题很简单,跟标题一样,是个热身题.要求生成一个5x5的单位矩阵 1A = eye(5); 完成 computeCost.m (计算单变量代价函数) 回想一下我们的单变量hypothesis公式 \[ h(\theta) = \theta_0 + \theta_1x \] 在这个公式中\(x\)指的是变量(variable),\(\theta_0\)和\(\theta_1\)是参数(parameter),之后的工作就是对代价函数(cost function)求导,找出使得代价函数最小(收敛时)\(\theta_0\)和\(\theta_1\)的值 刚开始学的时候有一段时间变量(variable)和参数(parameter)傻傻分不清 \(X\)是我们的数据矩阵,如下 \[ \left[ \begin{matrix} 3\\ 5\\ 8 \end{matrix} \right] \] 每一列代表一个特征,由于我们是单变量代价函数,所有X只有一列,为了不失一般性,可以改写我们的hypothesis公式 \[ h(\theta) = \theta_0x_0+ \theta_1x_1 \] 其中\(x_0\)的值始终为1,在我们的编程作业中\(\theta\)是一个向量代表\(\theta_1\)和\(\theta_2\),如下 \[ \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 为了用矩阵的乘法表示hypothesis公式,我们需要手动给\(X\)加上一列,如下(在ml编程作业中,已经帮我们添加了第一列1,所以无需我们再手动添加) \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] \] 然后执行矩阵的乘法 \[ h = X * \theta^T \] \[ \left[ \begin{matrix} 1 &amp; 3\\ 1 &amp; 5\\ 1 &amp; 8 \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] \] 代价函数 \[ J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}))^2 \] hypothesis计算完成后得到的是个m x 1的向量,代表的是我们通过h公式计算出的预测结果,m代表数据的行数,也就是\(X\)矩阵的行数.\(y\)向量代表的是真实的结果,所以\((h_\theta(x^{(i)}) - y^{(i)}))\)可以直接用两矩阵相减得出,即\(h - y\) 下面要做的就是计算平方和了,计算平方和有两种方法: \(\alpha^T * \alpha\) 永乐大帝在上,受小弟一拜 利用octave/matlib内置矩阵运算方法,先同时给所有数平方,再sum 于是,我们就可以得出我们的代价函数 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); 完成. gradientDescent.m (梯度下降) 为了使我们的线性方程方程\(h\)更贴近我们的数据集,因此我们要不断的去改变\(\theta_0\)和\(\theta_1\)这两个参数的值,使得代价函数\(J(\theta)\)取得最小值,如何取得最小值,我们用的是批量梯度下降算法(batch gradient descent algorithm) 回顾我们的批量梯度下降算法,即同时对每一个\(\theta\)求偏导,如下 \[ \theta = \theta - \alpha\cdot\frac{\partial}{\partial\theta}J(\theta) \] 即 \[ \theta_j = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j \] 由于我们是单变量,所以j只能取0,1,而且每次迭代我们必须要同时更新\(\theta_0\)和\(\theta_1\) (simultaneously update \(\theta_j\) for all j) 要计算 \((h_\theta(x^{(i)})-y^{(i)})\)的和,我们可以使用矩阵的乘法,由上文知\(X\)矩阵为m x 2 , \((h_\theta(x^{(i)})-y^{(i)})\) 为m维向量,则 \[ \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j = X^T*(h_\theta(x^{(i)})-y^{(i)}) = \left[ \begin{matrix} 1 &amp; 1 &amp; 1\\ 3 &amp; 5 &amp; 8\\ \end{matrix} \right] * \left[ \begin{matrix} 0 \\ 0 \\ 0 \end{matrix} \right] \] 可以简单的理解为\(X^T\)矩阵的第一行就是计算\(\theta_0\),第二行就是计算\(\theta_1\),第n行就是计算\(\theta_n\)(n &gt; 1就是多变量线性回归了,在这里我们的n = 1,也就是单变量线性回归) 因此,我们的代码可以这么写 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)); 完成 至此,我们的必修部分的编程作业都已做完,下面来看选修部分的作业 featureNormalize.m (特征标准化) 特征标准化的目的就是为了提升梯度下降的计算速度. 特征标准化包含两个方面: Feature Scaling Mean Normalize 它们将尝试使所有的特征都尽量放缩到-1到1之间(不严格,可有误差) 最简单的实现方式是\(x_n=\frac{x_n-\mu_n}{s_n}\),其中\(u_n\)是平均值,\(s_n\)是标准差 在octave中,一个mxn矩阵是可以和另一个m行的列向量或者n列的行向量相加减的,所以答案如下 123mu = mean(X);sigma = std(X);X_norm = (X_norm - mu) ./ sigma; 完成 computeCostMulti.m 由于矩阵乘法的原因,多变量线性回归的代价函数代码和单变量线性回归的代价函数代码一模一样,不再赘述 12h = X * theta;J = 1 / (2 * m) * sum((h - y) .^ 2); gradientDescentMulti.m 同理,多变量梯度下降的代码和单变量梯度下降的代码一样,不再赘述 12h = X * theta;theta = theta - alpha / m * (X&apos; * (h - y)) ; 完成 normalEqn.m(正规方程) 正规方程适用于特征个数不是特别多的场景(\(x&lt;10,000\)) , 优点: 不用迭代,一次即可计算出结果 不用选取学习率\(\alpha\)(learning rate) 不需要特征放缩 缺点: - 对于特征值个数\(x&gt;10,000\)时计算速度缓慢 - 只适用于线性回归模型,不适用于逻辑回归模型 - 可能出现矩阵不可逆的情况 公式: \[ (X*X^T)^{-1}X^Ty \] 由于公式本来就是用矩阵表示,所以用octave/matlib实现起来一点都不难,代码如下: 1theta = pinv(X&apos; * X) * X&apos; * y; 完成 最后,提交作业]]></content>
      <categories>
        <category>ml编程作业</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>吴恩达</tag>
        <tag>Andrew Ng</tag>
        <tag>编程作业</tag>
        <tag>machine-learning-ex1</tag>
      </tags>
  </entry>
</search>
